{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5953990",
   "metadata": {},
   "source": [
    "# Whale Vocalization Parsing\n",
    "\n",
    "This notebook contains the following functionalities:\n",
    "\n",
    "- Visualizing Data (time- and frequency-domain representations) \n",
    "- Dynamic time warping between two signals\n",
    "- Cross correlation between two signals\n",
    "- Monophonic pitch detection (zero-crossing, FFT peak, autocorrelation)\n",
    "- Signal denoising (on a per signal basis)\n",
    "- Locating probe signal in query for n-gram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed64b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell loads packages:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "import scipy.io.wavfile as wav\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import LinearColorMapper, ColorBar\n",
    "from bokeh.transform import linear_cmap\n",
    "import librosa\n",
    "from scipy.interpolate import interp1d \n",
    "from fastdtw import fastdtw\n",
    "import sklearn\n",
    "import os\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline \n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32925938",
   "metadata": {},
   "source": [
    "# Audio Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell loads audio:\n",
    "\n",
    "def load_audio(filename):\n",
    "    \n",
    "    # read in file and convert to range [-1, 1]:\n",
    "    \n",
    "    srate, audio = wav.read(filename)\n",
    "    audio = audio.astype(np.float32) / 32767.0 \n",
    "    \n",
    "    # set max to 0.9:\n",
    "    \n",
    "    if (len(audio.shape) == 1): \n",
    "        audio = (0.9 / max(np.abs(audio)) * audio)\n",
    "    else: \n",
    "        audio[:,0] = (0.9 / max(np.abs(audio[:,0])) * audio[:,0])\n",
    "        audio[:,1] = (0.9 / max(np.abs(audio[:,1])) * audio[:,1])\n",
    "        return audio.transpose(), srate\n",
    "    \n",
    "    # return audio:\n",
    "    \n",
    "    return audio, srate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6f34f",
   "metadata": {},
   "source": [
    "# Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccecfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the time-domain representation of two audio files:\n",
    "\n",
    "def plot_time_domain(audio, srate): \n",
    "    p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude')\n",
    "    time = np.linspace(0, len(audio)/srate, num=len(audio))\n",
    "    p.line(time, audio)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell plots the frequency-domain representation of two audio files:\n",
    "\n",
    "def plot_freq_domain(audio, srate):\n",
    "    f, t, s = signal.spectrogram(audio, srate)\n",
    "    s = 10 * np.log10(s + 1e-40)\n",
    "    p = figure(plot_width=800, plot_height=400, x_axis_label='Time (s)', y_axis_label='Frequency (Hz)')\n",
    "    p.image(image=[s], x=0, y=0, dw=t[-1], dh=f[-1], palette=\"Viridis256\", level=\"image\")\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba547a",
   "metadata": {},
   "source": [
    "# Audio Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell denoises an audio file:\n",
    "\n",
    "def denoise(\n",
    "    audio, \n",
    "    noise, \n",
    "    n_grad_freq   = 3,\n",
    "    n_grad_time   = 4,\n",
    "    n_fft         = 2048,\n",
    "    win_length    = 2048,\n",
    "    hop_length    = 512,\n",
    "    n_std_thresh  = 1,\n",
    "    prop_decrease = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    \n",
    "    Remove noise from audio based upon a clip containing only noise\n",
    "\n",
    "    Args:\n",
    "        audio (array)        : audio to denoise\n",
    "        noise (array)        : noise sample\n",
    "        n_grad_freq (int)    : how many frequency channels to smooth over with the mask.\n",
    "        n_grad_time (int)    : how many time channels to smooth over with the mask.\n",
    "        n_fft (int)          : number audio of frames between STFT columns.\n",
    "        win_length (int)     : Each frame of audio is windowed by `window()`. The window will be of length `win_length` and then padded with zeros to match `n_fft`..\n",
    "        hop_length (int)     : number audio of frames between STFT columns.\n",
    "        n_std_thresh (int)   : how many standard deviations louder than the mean dB of the noise (at each frequency level) to be considered signal\n",
    "\n",
    "    Returns:\n",
    "        \n",
    "        (array) The recovered signal with noise subtracted\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # STFT over noise:\n",
    "    \n",
    "    stft_noise    = librosa.stft(y = noise, n_fft = n_fft, hop_length = hop_length, win_length = win_length)\n",
    "    stft_noise_db = librosa.core.amplitude_to_db(np.abs(stft_noise))\n",
    "    \n",
    "    # calculate statistics over noise and noise threshold, over frequency axis:\n",
    "    \n",
    "    noise_freq_mean = np.mean(stft_noise_db, axis=1)\n",
    "    noise_freq_std  = np.std(stft_noise_db, axis=1)\n",
    "    noise_thresh    = noise_freq_mean + noise_freq_std * n_std_thresh\n",
    "    \n",
    "    # STFT over signal:\n",
    "\n",
    "    stft_audio    = librosa.stft(y = audio, n_fft = n_fft, hop_length = hop_length, win_length = win_length)\n",
    "    stft_audio_db = librosa.core.amplitude_to_db(np.abs(stft_audio))\n",
    "    \n",
    "    # calculate value to mask dB to:\n",
    "    \n",
    "    mask_gain_dB = np.min(stft_audio_db)\n",
    "    \n",
    "    # create a smoothing filter for the mask in time and frequency:\n",
    "    \n",
    "    smoothing_filter = np.outer(\n",
    "        np.concatenate(\n",
    "            [\n",
    "                np.linspace(0, 1, n_grad_freq + 1, endpoint=False),\n",
    "                np.linspace(1, 0, n_grad_freq + 2),\n",
    "            ]\n",
    "        )[1:-1],\n",
    "        np.concatenate(\n",
    "            [\n",
    "                np.linspace(0, 1, n_grad_time + 1, endpoint=False),\n",
    "                np.linspace(1, 0, n_grad_time + 2),\n",
    "            ]\n",
    "        )[1:-1],\n",
    "    )\n",
    "    smoothing_filter = smoothing_filter / np.sum(smoothing_filter)\n",
    "    \n",
    "    # calculate the threshold for each frequency/time bin:\n",
    "    \n",
    "    db_thresh = np.repeat(\n",
    "        np.reshape(noise_thresh, [1, len(noise_freq_mean)]),\n",
    "        np.shape(stft_audio_db)[1],\n",
    "        axis=0,\n",
    "    ).T\n",
    "    \n",
    "    # mask if the signal is above the threshold:\n",
    "    \n",
    "    mask_audio = stft_audio_db < db_thresh\n",
    "\n",
    "    # convolve the mask with a smoothing filter:\n",
    "    \n",
    "    mask_audio = scipy.signal.fftconvolve(mask_audio, smoothing_filter, mode = \"same\")\n",
    "    mask_audio = mask_audio * prop_decrease\n",
    "\n",
    "    # mask the signal:\n",
    "    \n",
    "    stft_audio_db_masked = (stft_audio_db * (1 - mask_audio) + np.ones(np.shape(mask_gain_dB)) \n",
    "                            * mask_gain_dB * mask_audio)\n",
    "    \n",
    "    # mask real:\n",
    "    \n",
    "    audio_imag_masked = np.imag(stft_audio) * (1 - mask_audio)\n",
    "    stft_audio_amp = (librosa.core.db_to_amplitude(stft_audio_db_masked) * np.sign(stft_audio)) + (\n",
    "        1j * audio_imag_masked\n",
    "    )\n",
    "\n",
    "    # recover the signal:\n",
    "    \n",
    "    recovered_signal = librosa.istft(stft_audio_amp, hop_length = hop_length, win_length = win_length)\n",
    "\n",
    "    return recovered_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7064a",
   "metadata": {},
   "source": [
    "# Audio Cross-Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell computes and visualizes the cross-correlation (or autocorrelation) between two signals:\n",
    "\n",
    "def correlation(audio_probe, audio_query, srate):\n",
    "    \n",
    "    xcorrelation = np.correlate(audio_probe, audio_query, mode = 'full')\n",
    "    \n",
    "    p = figure(plot_width=800, plot_height=200, x_axis_label='Delay (s)', y_axis_label='Cross-Correlation')\n",
    "    delay = np.linspace(0, len(audio_query)/srate, num=len(audio_query))\n",
    "    p.line(delay, xcorrelation)\n",
    "    show(p)\n",
    "    \n",
    "    return xcorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca327f78",
   "metadata": {},
   "source": [
    "# Audio Monophonic Pitch Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell implements different monophonic pitch detection methods:\n",
    "\n",
    "def pitch_zero_crossings(frame, srate): \n",
    "    \n",
    "    zero_indices   = np.nonzero((frame[1:] >= 0) & (frame[:-1] < 0))[0]\n",
    "    pitch_estimate = (srate / np.mean(np.diff(indices)))\n",
    "    \n",
    "    return pitch_estimate \n",
    "\n",
    "def pitch_fft(frame, srate): \n",
    "    \n",
    "    mag            = np.abs(np.fft.fft(frame))\n",
    "    mag            = mag[0:int(len(mag)/2)]\n",
    "    pitch_estimate = np.argmax(mag) * (srate / len(frame))\n",
    "    \n",
    "    return pitch_estimate \n",
    "\n",
    "def pitch_autocorrelation(frame, srate):\n",
    "    \n",
    "    xcorrelation   = np.correlate(frame, frame, mode = 'full')\n",
    "    derivative     = np.diff(xcorrelation[:int(len(xcorrelation)/2)+2])\n",
    "    peak_indices   = np.nonzero((derivative[:-1] > 0) & (derivative[1:] <= 0))[0] + 1\n",
    "    peak_values    = xcorrelation[peak_indices]\n",
    "    peak_indices_sorted = peak_indices[np.argsort(peak_values)[-2:]]\n",
    "    \n",
    "    return srate/(peak_indices_sorted[1]-peak_indices_sorted[0])\n",
    "\n",
    "def pitch_track(signal, hopSize, winSize, extractor, srate): \n",
    "    \n",
    "    offsets = np.arange(0, len(signal), hopSize)\n",
    "    pitch_track = np.zeros(len(offsets))\n",
    "    amp_track = np.zeros(len(offsets))\n",
    "    \n",
    "    for (m, o) in enumerate(offsets): \n",
    "        frame = signal[o:o+winSize] \n",
    "        pitch_track[m] = extractor(frame, srate)\n",
    "        amp_track[m] = np.sqrt(np.mean(np.square(frame)))  \n",
    "\n",
    "        if (pitch_track[m] > 1500): \n",
    "            pitch_track[m] = 0 \n",
    "    \n",
    "    return (amp_track, pitch_track)\n",
    "\n",
    "def sonify(amp_track, pitch_track, srate, hop_size):\n",
    "\n",
    "    times = np.arange(0.0, float(hop_size * len(pitch_track)) / srate,\n",
    "                      float(hop_size) / srate)\n",
    "\n",
    "    # sample locations in time (seconds)                                                      \n",
    "    sample_times = np.linspace(0, np.max(times), int(np.max(times)*srate-1))\n",
    "\n",
    "    freq_interpolator = interp1d(times,pitch_track)\n",
    "    amp_interpolator = interp1d(times,amp_track)\n",
    "                                                                \n",
    "    sample_freqs = freq_interpolator(sample_times)\n",
    "    sample_amps  = amp_interpolator(sample_times)\n",
    "\n",
    "    audio = np.zeros(len(sample_times));\n",
    "    T = 1.0 / srate\n",
    "    phase = 0.0\n",
    "    \n",
    "    for i in range(1, len(audio)):\n",
    "        audio[i] = sample_amps[i] * np.sin(phase)\n",
    "        phase = phase + (2*np.pi*T*sample_freqs[i])\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f2744",
   "metadata": {},
   "source": [
    "# Audio Dynamic Time Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd573b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell computes the dynamic time warping between two audio signals\n",
    "\n",
    "def dtw_table(x, y, distance = None):\n",
    "    \n",
    "    if distance is None:\n",
    "        distance = scipy.spatial.distance.euclidean\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    table = np.zeros((nx+1, ny+1))\n",
    "    \n",
    "    # compute left column separately, i.e. j=0.\n",
    "    table[1:, 0] = np.inf\n",
    "        \n",
    "    # compute top row separately, i.e. i=0.\n",
    "    table[0, 1:] = np.inf\n",
    "        \n",
    "    # Fill in the rest.\n",
    "    for i in range(1, nx+1):\n",
    "        for j in range(1, ny+1):\n",
    "            d = distance(x[i-1], y[j-1])\n",
    "            table[i, j] = d + min(table[i-1, j], table[i, j-1], table[i-1, j-1])\n",
    "    return table\n",
    "\n",
    "# dtw table traceback function:\n",
    "\n",
    "def dtw_path(x, y, table):\n",
    "    \n",
    "    i = len(x)\n",
    "    j = len(y)\n",
    "    path = [(i, j)]\n",
    "    while i > 0 or j > 0:\n",
    "        minval = np.inf\n",
    "        if table[i-1][j-1] < minval:\n",
    "            minval = table[i-1, j-1]\n",
    "            step = (i-1, j-1)\n",
    "        if table[i-1, j] < minval:\n",
    "            minval = table[i-1, j]\n",
    "            step = (i-1, j)\n",
    "        if table[i][j-1] < minval:\n",
    "            minval = table[i, j-1]\n",
    "            step = (i, j-1)\n",
    "        path.insert(0, step)\n",
    "        i, j = step\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# plot dtw table and path, along with signals:\n",
    "\n",
    "def plot_dtw(table, path, signal1, signal2):\n",
    "    \n",
    "    %matplotlib widget\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "\n",
    "    # Bottom right plot.\n",
    "    ax1 = plt.axes([0.2, 0, 0.8, 0.2])\n",
    "    ax1.imshow(signal1, origin = 'upper', aspect = 'auto', cmap = 'coolwarm')\n",
    "    ax1.set_xlabel('Signal 1')\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_ylim(10)\n",
    "\n",
    "    # Top left plot.\n",
    "    ax2 = plt.axes([0, 0.2, 0.20, 0.8])\n",
    "    ax2.imshow(signal2.T, origin = 'lower', aspect = 'auto', cmap = 'coolwarm')\n",
    "    ax2.set_ylabel('Signal 2')\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_ylim(1)\n",
    "\n",
    "    # Top right plot.\n",
    "    ax3 = plt.axes([0.2, 0.2, 0.8, 0.8], sharex = ax1, sharey = ax2)\n",
    "    ax3.imshow(table.T, aspect = 'auto', origin = 'upper', interpolation = 'nearest', cmap = 'gray')\n",
    "    ax3.set_xticks([])\n",
    "    ax3.set_yticks([])\n",
    "\n",
    "    # Path.\n",
    "    ax3.plot(path[:,0], path[:,1], 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bac0fd",
   "metadata": {},
   "source": [
    "# Audio Isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell trims a denoised call file based on a amplitude/pitch track version:\n",
    "\n",
    "def trim(audio, amplitude_track, srate, binSize = 6, hopSize = 256, threshold = 2.5, sensitivity = 5):\n",
    "\n",
    "    # get noise at beginning and end of audio clip: \n",
    "    \n",
    "    begin_noise_level = np.mean(amplitude_track[:int(len(amplitude_track)/binSize)])\n",
    "    end_noise_level   = np.mean(amplitude_track[-int(len(amplitude_track)/binSize):])\n",
    "        \n",
    "    index_first = None\n",
    "    index_last  = None\n",
    "    \n",
    "    # if signal > threshold * begin_noise_level many times in a row, trim here (from track start):\n",
    "    \n",
    "    for i in range(len(amplitude_track)):\n",
    "        if amplitude_track[i] > threshold * begin_noise_level:\n",
    "            if all(amplitude_track[j] > threshold * begin_noise_level for j in range(i, i + sensitivity + 1)):\n",
    "                index_first = i\n",
    "                break\n",
    "    \n",
    "    # if signal > threshold * end_noise_level many times in a row, trim here (from track end):\n",
    "\n",
    "    for i in range(len(amplitude_track)):\n",
    "        if amplitude_track[i] > threshold * end_noise_level:\n",
    "            if all(amplitude_track[j] > threshold * end_noise_level for j in range(i - sensitivity + 2, i + 1)):\n",
    "                index_last = i\n",
    "    \n",
    "    return audio[index_first * hopSize:(index_last + 10) * hopSize]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a6897",
   "metadata": {},
   "source": [
    "# Audio Probe Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e1e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes a probe sequence and a query sequence, and locates the probe in the query:\n",
    "\n",
    "def probe_localization(probe_audio, query_audio, winSize, srate, threshold):\n",
    "    \n",
    "    # get correlation between probe and query, and probe and probe:\n",
    "    \n",
    "    correlation_pq = np.correlate(probe_audio, query_audio, mode = 'same')[::-1]\n",
    "    correlation_pp = np.correlate(probe_audio, probe_audio, mode = 'same')\n",
    "    \n",
    "    # define localization threshold based on probe-probe correlation:\n",
    "    \n",
    "    thresh = threshold * max(correlation_pp)\n",
    "\n",
    "    offsets = np.arange(0, len(correlation_pq), winSize)\n",
    "    amp = np.zeros(len(offsets))\n",
    "    \n",
    "    for (m, o) in enumerate(offsets): \n",
    "        frame = correlation_pq[o:o+winSize] \n",
    "        amp[m] = np.max(np.abs(frame))\n",
    "    \n",
    "    localizations = np.array([i for i in range(1, len(amp) - 1) \n",
    "                              if (amp[i] > amp[i - 1] and amp[i] > amp[i + 1] and amp[i] > thresh)])\n",
    "    \n",
    "    return amp, localizations * winSize / srate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b1fea5",
   "metadata": {},
   "source": [
    "# Audio Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell measures the similarity between two calls, using a variety of methods:\n",
    "\n",
    "# cross-correlation peak max:\n",
    "\n",
    "def similarity_cross_correlation(audio_1, audio_2):\n",
    "    \n",
    "    correlation = np.correlate(audio_1, audio_2, mode = \"same\")\n",
    "    \n",
    "    return max(correlation)\n",
    "\n",
    "# dtw distance:\n",
    "\n",
    "def similarity_dynamic_time_warping(audio_1, audio_2):\n",
    "    \n",
    "    distance, path = fastdtw(call_1, call_2)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# mfcc distance:\n",
    "\n",
    "def similarity_mfcc(audio_1, audio_2, srate):\n",
    "    \n",
    "    mfccs_1 = librosa.feature.mfcc(y = audio_1, sr = srate)\n",
    "    mfccs_2 = librosa.feature.mfcc(y = audio_2, sr = srate)\n",
    "    \n",
    "    distance = np.linalg.norm(mfccs_1 - mfccs_2)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a28d80",
   "metadata": {},
   "source": [
    "# Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1970a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_file, srate):\n",
    "\n",
    "    # initialize features dictionary:\n",
    "    \n",
    "    features_dict = {}\n",
    "\n",
    "    # spectral features:\n",
    "    \n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y = audio_file, sr = srate)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y = audio_file, sr = srate)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y = audio_file, sr = srate)\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y = audio_file)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y = audio_file, sr = srate)\n",
    "    \n",
    "    features_dict['spectral_centroid_mean'] = spectral_centroid.mean()\n",
    "    features_dict['spectral_bandwidth_mean'] = spectral_bandwidth.mean()\n",
    "    features_dict['spectral_contrast_mean'] = spectral_contrast.mean()\n",
    "    features_dict['spectral_flatness_mean'] = spectral_flatness.mean()\n",
    "    features_dict['spectral_rolloff_mean'] = spectral_rolloff.mean()\n",
    "    \n",
    "    features_dict['spectral_centroid_std'] = spectral_centroid.std()\n",
    "    features_dict['spectral_bandwidth_std'] = spectral_bandwidth.std()\n",
    "    features_dict['spectral_contrast_std'] = spectral_contrast.std()\n",
    "    features_dict['spectral_flatness_std'] = spectral_flatness.std()\n",
    "    features_dict['spectral_rolloff_std'] = spectral_rolloff.std()\n",
    "\n",
    "    # temporal features:\n",
    "    \n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y = audio_file)\n",
    "    rmse = librosa.feature.rms(y = audio_file)\n",
    "    \n",
    "    features_dict['zero_crossing_rate_mean'] = zero_crossing_rate.mean()\n",
    "    features_dict['rmse_mean'] = rmse.mean()\n",
    "\n",
    "    features_dict['zero_crossing_rate_std'] = zero_crossing_rate.std()\n",
    "    features_dict['rmse_std'] = rmse.std()\n",
    "    \n",
    "    # MFCCs:\n",
    "    \n",
    "    mfccs = librosa.feature.mfcc(y = audio_file, sr = srate)\n",
    "    for i in range(mfccs.shape[0]):\n",
    "        features_dict[f'mfcc_{i+1}'] = mfccs[i].mean()\n",
    "\n",
    "    # Additional features\n",
    "    chroma = librosa.feature.chroma_stft(y = audio_file, sr = srate)\n",
    "    tempogram = librosa.feature.tempogram(y = audio_file, sr = srate)\n",
    "    tonnetz = librosa.feature.tonnetz(y = audio_file, sr = srate)\n",
    "    \n",
    "    for i in range(chroma.shape[0]):\n",
    "        features_dict[f'chroma_{i+1}'] = chroma[i].mean()\n",
    "        \n",
    "    for i in range(tempogram.shape[0]):\n",
    "        features_dict[f'tempogram_{i+1}'] = tempogram[i].mean()\n",
    "        \n",
    "    for i in range(tonnetz.shape[0]):\n",
    "        features_dict[f'tonnetz_{i+1}'] = tonnetz[i].mean()\n",
    "\n",
    "    return features_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d159a8",
   "metadata": {},
   "source": [
    "# Audio Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell takes an array of feature vectors, and clusters them:\n",
    "\n",
    "def cluster_adaptive_kmeans(data, max_clusters = 50, tolerance = 0.01):\n",
    "    \n",
    "    kmeans = sklearn.cluster.Kmeans(n_clusters = 10, random_state = 0).fit(data)\n",
    "    prev_inertia = kmeans.inertia_\n",
    "    \n",
    "    for n_clusters in range(11, max_clusters + 1):\n",
    "        \n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = 0).fit(data)\n",
    "        inertia = kmeans.inertia_\n",
    "        \n",
    "        if (prev_inertia - inertia) / prev_inertia < tolerance:\n",
    "            break\n",
    "        \n",
    "        prev_inertia = inertia\n",
    "    \n",
    "    return kmeans\n",
    "\n",
    "def cluster_dbscan(data, eps = 0.5, min_samples = 5):\n",
    "    \n",
    "    dbscan = sklearn.cluster.DBSCAN(eps = eps, min_samples = min_samples)    \n",
    "    dbscan.fit(data)\n",
    "    labels = dbscan.labels_\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "    \n",
    "    return labels, n_clusters_, n_noise_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce82988",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ca728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load two calls:\n",
    "\n",
    "(whale_1, srate) = load_audio('C:/Users/tsmza/Desktop/school/course/CSC575/project/whales/data/call-catalog/pretty/norm/A04-honk-070706-D011-10255.wav')\n",
    "(whale_2, srate) = load_audio('C:/Users/tsmza/Desktop/school/course/CSC575/project/whales/data/call-catalog/pretty/norm/A04-honk-070706-D011-10359.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b1544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 (original):\n",
    "\n",
    "plot_time_domain(whale_1, srate)\n",
    "plot_freq_domain(whale_1, srate)\n",
    "ipd.Audio(whale_1, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ea4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 (denoised):\n",
    "\n",
    "whale_1_denoised = denoise(whale_1, whale_1[:int(len(whale_1)/10)])\n",
    "plot_time_domain(whale_1_denoised, srate)\n",
    "plot_freq_domain(whale_1_denoised, srate)\n",
    "ipd.Audio(whale_1_denoised, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ce786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 (original):\n",
    "\n",
    "plot_time_domain(whale_2, srate)\n",
    "plot_freq_domain(whale_2, srate)\n",
    "ipd.Audio(whale_2, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8443ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 (denoised):\n",
    "\n",
    "whale_2_denoised = denoise(whale_2, whale_2[:int(len(whale_2)/10)])\n",
    "plot_time_domain(whale_2_denoised, srate)\n",
    "plot_freq_domain(whale_2_denoised, srate)\n",
    "ipd.Audio(whale_2_denoised, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 monophonic pitch detection:\n",
    "\n",
    "whale_1_amp, whale_1_pitch  = pitch_track(whale_1_denoised, 256, 512, pitch_autocorrelation)\n",
    "whale_1_pitch_filtered      = signal.medfilt(whale_1_pitch, kernel_size=15)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Pitch (Extracted)')\n",
    "time = np.linspace(0, len(whale_1)/srate, num=len(whale_1_pitch_filtered))\n",
    "p.line(time, whale_1_pitch_filtered)\n",
    "show(p)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude (Extracted)')\n",
    "time = np.linspace(0, len(whale_1)/srate, num=len(whale_1_pitch))\n",
    "p.line(time, whale_1_amp)\n",
    "show(p)\n",
    "\n",
    "whale_1_mpd                 = sonify(whale_1_amp, whale_1_pitch_filtered, srate, 256)\n",
    "ipd.Audio(whale_1_mpd, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e4c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 monophonic pitch detection:\n",
    "\n",
    "whale_2_amp, whale_2_pitch  = pitch_track(whale_2_denoised, 256, 512, pitch_autocorrelation)\n",
    "whale_2_pitch_filtered      = signal.medfilt(whale_2_pitch, kernel_size=15)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Pitch (Extracted)')\n",
    "time = np.linspace(0, len(whale_2)/srate, num=len(whale_2_pitch_filtered))\n",
    "p.line(time, whale_2_pitch_filtered)\n",
    "show(p)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude (Extracted)')\n",
    "time = np.linspace(0, len(whale_2)/srate, num=len(whale_2_pitch))\n",
    "p.line(time, whale_2_amp)\n",
    "show(p)\n",
    "\n",
    "whale_2_mpd                 = sonify(whale_2_amp, whale_2_pitch_filtered, srate, 256)\n",
    "ipd.Audio(whale_2_mpd, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0841f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_1_stft = librosa.feature.chroma_stft(y = whale_1_denoised, sr = srate, hop_length = 256)\n",
    "whale_2_stft = librosa.feature.chroma_stft(y = whale_2_denoised, sr = srate, hop_length = 256)\n",
    "\n",
    "distance = dtw_table(whale_1_stft.T, whale_2_stft.T, distance = scipy.spatial.distance.cosine)\n",
    "path     = dtw_path(whale_1_stft.T, whale_2_stft.T, distance)\n",
    "\n",
    "plot_dtw(distance, path, whale_1_stft, whale_2_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36770601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 1 denoised and trimmed:\n",
    "\n",
    "whale_1_denoised_trimmed = trim(whale_1_denoised, whale_1_amp, srate, sensitivity = 10)\n",
    "plot_time_domain(whale_1_denoised_trimmed, srate)\n",
    "plot_freq_domain(whale_1_denoised_trimmed, srate)\n",
    "ipd.Audio(whale_1_denoised_trimmed, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c0dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call 2 denoised and trimmed:\n",
    "\n",
    "whale_2_denoised_trimmed = trim(whale_2_denoised, whale_2_amp, srate, sensitivity = 10)\n",
    "plot_time_domain(whale_2_denoised_trimmed, srate)\n",
    "plot_freq_domain(whale_2_denoised_trimmed, srate)\n",
    "ipd.Audio(whale_2_denoised_trimmed, rate = srate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bcd2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_1_trimmed_stft = librosa.feature.chroma_stft(y = whale_1_denoised_trimmed, sr = srate, hop_length = 256)\n",
    "whale_2_trimmed_stft = librosa.feature.chroma_stft(y = whale_2_denoised_trimmed, sr = srate, hop_length = 256)\n",
    "\n",
    "distance = dtw_table(whale_1_trimmed_stft.T, whale_2_trimmed_stft.T, distance = scipy.spatial.distance.cosine)\n",
    "path     = dtw_path(whale_1_trimmed_stft.T, whale_2_trimmed_stft.T, distance)\n",
    "\n",
    "plot_dtw(distance, path, whale_1_trimmed_stft, whale_2_trimmed_stft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b830cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a fake query sequence and search for probe in query:\n",
    "\n",
    "query = np.concatenate((whale_1_denoised, whale_2_denoised, whale_2_denoised))\n",
    "probe = whale_1_denoised\n",
    "\n",
    "localization = probe_localization(probe, query, 22050, 44100, 0.2)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Time (s)', y_axis_label='Amplitude')\n",
    "p.line(np.linspace(0, len(query)/44100, len(query)), query)\n",
    "show(p)\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, x_axis_label='Delay (s)', y_axis_label='Cross-Correlation')\n",
    "p.line(np.linspace(0, int(len(localization[0]) * 22050 / 44100), len(localization[0])), localization[0])\n",
    "show(p)\n",
    "\n",
    "print(\"Calls located at: \", *localization[1], \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad34b44",
   "metadata": {},
   "source": [
    "## Call Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05026182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve audio file names, pods, call types:\n",
    "\n",
    "files = os.listdir('c:/users/tsmza/desktop/school/course/csc575/project/whales/data/call-catalog/orig/wav')\n",
    "\n",
    "pods  = np.array([string.split('-')[0] for string in files])\n",
    "types = np.array([string.split('-')[1] for string in files])\n",
    "\n",
    "pods_unique  = np.unique(pods)\n",
    "types_unique = np.unique(types)\n",
    "\n",
    "files_according_to_pod  = {x.split('-')[0]: [f for f in files if f.startswith(x.split('-')[0])] for x in files}\n",
    "files_according_to_type = {x.split('-')[1]: [f for f in files if f.split('-')[1] == x.split('-')[1]] for x in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all files, denoise, and trim:\n",
    "\n",
    "for i in files:\n",
    "    \n",
    "    try: \n",
    "\n",
    "        # load audio:\n",
    "\n",
    "        audio = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/call-catalog/orig/wav/\"+i)\n",
    "\n",
    "        # denoise:\n",
    "\n",
    "        audio_denoised = denoise(audio[0], audio[0][:int(len(audio[0])/10)])\n",
    "\n",
    "        # trim:\n",
    "\n",
    "        audio_denoised_amp, audio_denoised_pitch  = pitch_track(audio_denoised, 256, 512, pitch_autocorrelation, audio[1])\n",
    "        audio_denoised_trimmed = trim(audio_denoised, audio_denoised_amp, audio[1], sensitivity = 10)\n",
    "\n",
    "        # output:\n",
    "\n",
    "        sf.write(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+i, audio_denoised_trimmed, audio[1])\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0365b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate global call variances across pods:\n",
    "\n",
    "features_mean = []\n",
    "features_sdev = []\n",
    "\n",
    "for key, values in files_according_to_type.items():\n",
    "        \n",
    "    length = []\n",
    "    freq   = []\n",
    "    \n",
    "    for filename in values:\n",
    "        \n",
    "        try:\n",
    "        \n",
    "            audio = load_audio(\"c:/users/tsmza/desktop/school/course/csc575/project/whales/data/data/\"+filename)\n",
    "            audio_fft   = np.fft.fft(audio[0])\n",
    "            audio_freqs = np.fft.fftfreq(len(audio_fft), 1 / audio[1])\n",
    "\n",
    "            length.append(len(audio[0])/audio[1])\n",
    "            freq.append(audio_freqs[np.argmax(np.abs(audio_fft))])\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "    features_mean.append([key, np.mean(length), np.mean(freq)])\n",
    "    features_sdev.append([key, np.std(length), np.std(freq)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843dab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
